{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriciosantana/nlp/blob/main/AKCIT_NLP_M6_Colab_Unidade_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfevqw7Eyf76"
      },
      "source": [
        "Essa aula se baseia:\n",
        "\n",
        "\n",
        "\n",
        "*   No artigo \"*Distributed Representations of Words and Phrases\n",
        "and their Compositionality*\", de Mikolov et al. (2013). Clique [aqui](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) para acesso.\n",
        "*   Na documentação do word2vec da biblioteca [Gensim](https://radimrehurek.com/gensim/models/word2vec.html).\n",
        "# Objetivos de Aprendizagem\n",
        "\n",
        "Neste notebook mostramos como treinar um modelo Word2Vec do zero e como importar um modelo pré-treinado usando a biblioteca gensim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJA5knmZwYFH"
      },
      "source": [
        "\n",
        "## Princípios de Word Embeddings\n",
        "\n",
        "Inicialmente é necessário instalar as bibliotecas keras, tensorflow, gensim e np_utils. Para instalar esses recursos você usará o comeando !pip install biblioteca\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_2HM5KZ8GfH",
        "outputId": "85a4f2ee-a729-4b38-baf2-7cf597ec990d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Collecting np_utils\n",
            "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m567.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from np_utils) (1.26.4)\n",
            "Building wheels for collected packages: np_utils\n",
            "  Building wheel for np_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np_utils: filename=np_utils-0.6.0-py3-none-any.whl size=56437 sha256=88b5e83a6ca29bdb51a65a8ef7dc4495bde0d49e5e932e03db3506b0d3c9f8da\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/c7/50/2307607f44366dd021209f660045f8d51cb976514d30be7cc7\n",
            "Successfully built np_utils\n",
            "Installing collected packages: np_utils\n",
            "Successfully installed np_utils-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow\n",
        "!pip install -U gensim\n",
        "!pip install np_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvZsiZ_9xRbd"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Embedding,Activation,Dropout,SimpleRNN,BatchNormalization,RNN,Flatten,Input,LSTM,Bidirectional\n",
        "#from keras.utils.np_utils import to_categorical\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7wDKHcspMsN"
      },
      "source": [
        "Caso você queira treinar um modelo Word2vec do Zero, a biblioteca Gensim provê essa funcionalide:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq7e6d-m7tlJ"
      },
      "source": [
        "Alguns comentários sobre o código a seguir:\n",
        "\n",
        "1)Observe que estamos um usanddo um corpus já tratado, com palavras de interesse\n",
        "(imagine que temos um corpus com várias sentenças já tokenizadas por espaços). Cada setença então está em um vetor e o corpus é um vetor de vetores. Entretanto, a entrada poderia vir de um arquivo no disco, da rede em tempo real, sem precisar carregar todo o seu corpus na RAM.\n",
        "\n",
        "2) O Word2vec aceita vários parâmetros que afetam tanto a velocidade quanto a qualidade do treinamento.\n",
        "\n",
        "3) Um deles é para podar o dicionário interno. Palavras que aparecem apenas uma ou duas vezes em um corpus de um bilhão de palavras são provavelmente erros de digitação desinteressantes e lixo. Além disso, não há dados suficientes para fazer qualquer treinamento significativo sobre essas palavras, então é melhor ignorá-las. Um valor razoável para min_count está entre 0-100, dependendo do tamanho do seu conjunto de dados.\n",
        "\n",
        "4) O paralelismo de treinamento é tratado no número de workers, para acelerar o treinamento, default = 1 -- worker = no parallelization\n",
        "\n",
        "5) vector_size (int, opcional) – Dimensionalidade dos vetores de palavras.\n",
        "\n",
        "Aqui temos a lista completa de parâmetros e valores default.\n",
        "https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
        "\n",
        "\n",
        "6) Observe a variável model resultado:  Este objeto contém essencialmente o mapeamento entre palavras e embeddings. Após o treinamento, ele pode ser usado diretamente para consultar esses embeddings de várias maneiras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU9SzLCHpLpp"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "corpus = [[\"hello\", \"world\",\"hi\",\"earth\",\"sunshine\",\"law\"],[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
        "model = Word2Vec(sentences=corpus, vector_size=5, window=5, min_count=1, workers=4)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaCOZiK9phDi"
      },
      "source": [
        "Os vetores de palavras treinados são armazenados em uma instância KeyedVectors, como model.wv. O motivo para separar os vetores treinados em KeyedVectors é que se você não precisar mais do estado completo do modelo (não precisar continuar o treinamento), seu estado pode ser descartado, mantendo apenas os vetores e suas chaves adequadas.\n",
        "\n",
        "\n",
        "Uma vez o modelo treinado podemos carregar esse modelo e realizar operações aritméticas entre termos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqZUI4AwpgdW"
      },
      "outputs": [],
      "source": [
        "word_vectors = model.wv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViRNHApXAqeQ"
      },
      "source": [
        "Observe que o tamanho dos vetores obtidos é 5, definido na etapa de treinamento. Temos um vetor denso de tamanho 5 para cada palavra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1m49eUXpv_8",
        "outputId": "109f8bf0-eab9-4cc5-eabd-77a5500389e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(word_vectors[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjA3IkGGqkMG"
      },
      "source": [
        "Aqui podemos verificar a operação de subtração dos vetores das palavras 'dog' e 'cat'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvOKe8VXqfrb",
        "outputId": "ca79b69d-f9ee-444a-f69d-7aedc1206abe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.00137478, -0.13207467, -0.22588612,  0.11582372, -0.22422102],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_vectors['dog'] - word_vectors['cat']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqUcZ3FCq-7p"
      },
      "source": [
        "Podemos ainda retreinar o modelo com novas palavras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOkz512_rD0c",
        "outputId": "dd8a8e0d-a6a4-4c2d-fc73-8347eec253bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
            "WARNING:gensim.models.word2vec:EPOCH 0: supplied example count (1) did not equal expected count (3)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0, 3)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.train([[\"dear\", \"bear\", \"cream\"]], total_examples=3, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11Q8BECRroHw"
      },
      "source": [
        "### Carregando Modelos Treinados em Outros Corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iusdjNyd9J9t"
      },
      "source": [
        "Existem diversos repositórios de *embeddings* pré-treinados para facilitar nossa vida. Algumas bibliotecas dispobilizam isso para nós usuários. Veja a senguir os modelos pré-treinados de *word embeddings* disponíveis na biblioteca gensim:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkF0clmPsF8e",
        "outputId": "ab60f4e3-bf32-4677-e401-58901fc10378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader\n",
        "\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMwWGYtEslt1"
      },
      "source": [
        "Existem vários, mas vamos escolher um modelo treinado no google *news* com 300 dimensões e no idioma Inglês. Observe que esse processo pode ser um pouco demorado em função do tamanho deste modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFsF-t0zsjCz",
        "outputId": "69995af6-3753-44fa-d592-a3679c300c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "word_vectors = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXJXLrXwtRNL"
      },
      "source": [
        "Agora, vamos usar este modelo? Quais são as palavras mais similares à palavra 'car'?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWGhm2eltX5M",
        "outputId": "d88cd37a-1276-48e6-c911-a5b092849c0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('vehicle', 0.7821096181869507),\n",
              " ('cars', 0.7423831224441528),\n",
              " ('SUV', 0.7160962224006653),\n",
              " ('minivan', 0.6907036900520325),\n",
              " ('truck', 0.6735789775848389),\n",
              " ('Car', 0.6677608489990234),\n",
              " ('Ford_Focus', 0.667320191860199),\n",
              " ('Honda_Civic', 0.6626849174499512),\n",
              " ('Jeep', 0.651133120059967),\n",
              " ('pickup_truck', 0.6441438794136047)]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_vectors.most_similar('car')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpIUnC4oumHw"
      },
      "source": [
        "Podemos realizar operaçõe aritméticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn3G47M3uqOV",
        "outputId": "7a0a4a7e-9ac6-485c-e8f5-c023893731c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 2.53417969e-01, -2.23144531e-01,  3.80859375e-02,  2.54394531e-01,\n",
              "       -1.92871094e-01, -1.28906250e-01, -3.61328125e-02,  2.16796875e-01,\n",
              "        2.08007812e-01,  1.16699219e-01,  1.44042969e-01, -1.10351562e-01,\n",
              "        9.17968750e-02, -2.11425781e-01, -6.05468750e-02,  1.19140625e-01,\n",
              "       -1.39404297e-01,  1.58508301e-01,  2.34375000e-02, -4.05273438e-02,\n",
              "        5.56640625e-02,  5.85021973e-02,  1.89697266e-01, -2.05078125e-02,\n",
              "        4.58526611e-03, -7.69042969e-02,  6.83593750e-02, -4.39453125e-03,\n",
              "       -7.51953125e-02, -1.25976562e-01, -2.45117188e-01, -1.58203125e-01,\n",
              "       -1.97753906e-01,  2.93212891e-01,  2.24243164e-01,  1.37695312e-01,\n",
              "        2.87658691e-01, -9.76562500e-02, -3.07617188e-02,  1.98242188e-01,\n",
              "        7.30438232e-02,  5.85937500e-03, -1.48437500e-01,  7.95288086e-02,\n",
              "       -5.29785156e-02,  1.97265625e-01, -1.09863281e-01, -2.58789062e-02,\n",
              "        1.17187500e-02, -1.85546875e-01, -1.45019531e-01,  2.28515625e-01,\n",
              "       -1.41601562e-02, -9.98535156e-02, -1.67724609e-01,  2.03125000e-01,\n",
              "        9.13085938e-02, -2.61230469e-02,  2.07031250e-01,  1.75781250e-02,\n",
              "        1.39648438e-01, -1.73828125e-01, -2.76184082e-01,  7.51953125e-02,\n",
              "        2.00683594e-01,  8.53271484e-02,  2.92968750e-03, -9.76562500e-03,\n",
              "       -2.62451172e-01,  6.56738281e-02, -4.34570312e-02, -2.16552734e-01,\n",
              "       -1.29638672e-01,  7.36083984e-02, -1.01562500e-01, -5.37109375e-03,\n",
              "       -2.14843750e-02, -2.11425781e-01,  1.34277344e-01, -6.15234375e-02,\n",
              "       -1.34277344e-02,  1.56250000e-02, -1.03637695e-01, -2.73437500e-02,\n",
              "       -1.20849609e-01, -4.49218750e-02,  1.76330566e-01,  1.81579590e-01,\n",
              "       -4.78515625e-02,  9.86328125e-02, -1.34033203e-01,  2.93823242e-01,\n",
              "        9.66796875e-02,  9.03320312e-02, -9.13085938e-02, -1.03515625e-01,\n",
              "        8.71582031e-02, -2.04833984e-01,  1.93847656e-01, -1.44531250e-01,\n",
              "       -1.75781250e-01, -2.55859375e-01,  1.42822266e-01,  2.44140625e-02,\n",
              "        1.67968750e-01,  1.46484375e-01, -1.06445312e-01, -8.44726562e-02,\n",
              "       -1.68945312e-01,  3.54003906e-03, -3.16406250e-01, -2.11914062e-01,\n",
              "       -6.73828125e-02, -1.05834961e-01,  2.36328125e-01, -4.95605469e-02,\n",
              "        1.13769531e-01,  6.54296875e-02, -6.00585938e-02,  4.74853516e-02,\n",
              "        1.48925781e-01, -9.57031250e-02,  3.22265625e-02, -3.43017578e-01,\n",
              "        3.70483398e-02,  9.75341797e-02,  6.54296875e-02, -8.83789062e-02,\n",
              "       -6.83593750e-03,  1.87500000e-01,  8.00781250e-02,  3.90625000e-02,\n",
              "       -1.22070312e-01,  3.85742188e-01,  1.62109375e-01,  2.34375000e-01,\n",
              "       -7.29980469e-02, -3.03710938e-01,  1.66503906e-01, -2.49267578e-01,\n",
              "       -6.84814453e-02,  1.04003906e-01, -1.38671875e-01,  2.44140625e-03,\n",
              "        1.80541992e-01, -2.34863281e-01,  8.00781250e-02, -1.07421875e-02,\n",
              "       -1.97753906e-02, -3.61328125e-02,  9.76562500e-03, -1.95800781e-01,\n",
              "       -1.48925781e-01, -1.61743164e-02,  2.73437500e-02,  5.04882812e-01,\n",
              "        8.78906250e-02,  1.20117188e-01, -1.45996094e-01,  1.81396484e-01,\n",
              "        1.85058594e-01,  1.46484375e-02, -1.32385254e-01, -1.41601562e-01,\n",
              "        2.20703125e-01,  1.08764648e-01,  4.88281250e-02, -6.68945312e-02,\n",
              "       -9.32617188e-02,  8.88671875e-02, -7.32421875e-02, -1.14501953e-01,\n",
              "       -1.86523438e-01, -1.39648438e-01, -3.39355469e-02, -1.99462891e-01,\n",
              "        1.95556641e-01, -1.27929688e-01, -7.61718750e-02, -4.62646484e-02,\n",
              "       -3.71093750e-02, -2.59765625e-01, -1.58935547e-01, -9.47265625e-02,\n",
              "        1.75781250e-02, -2.04589844e-01,  5.76171875e-02,  8.76464844e-02,\n",
              "       -1.56250000e-02,  1.55273438e-01,  1.21582031e-01,  9.66796875e-02,\n",
              "       -1.70410156e-01,  6.64062500e-02,  9.76562500e-04, -1.92382812e-01,\n",
              "        2.06298828e-01, -1.87011719e-01,  1.26953125e-02,  2.48046875e-01,\n",
              "       -8.00781250e-02, -1.65893555e-01, -2.44140625e-01, -1.33789062e-01,\n",
              "       -1.99279785e-01, -8.64257812e-02,  6.66503906e-02, -2.44140625e-04,\n",
              "       -1.01562500e-01,  2.05810547e-01,  1.00585938e-01, -2.05383301e-01,\n",
              "        1.38916016e-01,  4.23812866e-02,  6.44531250e-02,  5.22460938e-02,\n",
              "       -2.51464844e-01,  2.59765625e-01,  5.55877686e-02, -4.09240723e-02,\n",
              "        2.02636719e-01,  6.54296875e-02,  2.14843750e-02, -1.92871094e-01,\n",
              "        3.25195312e-01,  5.76171875e-02,  1.60156250e-01,  1.65039062e-01,\n",
              "       -1.46484375e-01, -6.62841797e-02, -1.80664062e-01,  1.41113281e-01,\n",
              "       -5.66406250e-02, -1.71875000e-01, -2.78808594e-01, -3.05175781e-02,\n",
              "       -8.78906250e-03,  5.65795898e-02,  7.95898438e-02,  2.35595703e-01,\n",
              "        1.85058594e-01, -1.04980469e-02, -1.95800781e-01,  2.65625000e-01,\n",
              "        2.26684570e-01, -1.32812500e-01, -2.53906250e-02,  1.38671875e-01,\n",
              "        1.33789062e-01, -3.32031250e-01, -9.93652344e-02, -8.00781250e-02,\n",
              "        1.96777344e-01, -1.51855469e-01,  2.70996094e-02, -6.39648438e-02,\n",
              "        1.12304688e-02, -1.68945312e-01,  2.79541016e-02, -1.98730469e-01,\n",
              "        9.52148438e-03, -1.62109375e-01,  4.42504883e-02, -7.81250000e-02,\n",
              "       -7.71484375e-02,  1.07421875e-01, -1.97631836e-01,  3.32031250e-02,\n",
              "       -3.12500000e-02,  1.65100098e-01,  6.83593750e-03,  8.78906250e-03,\n",
              "        3.05664062e-01, -2.05444336e-01, -2.72949219e-01, -9.66796875e-02,\n",
              "        1.91375732e-01, -7.34863281e-02,  2.92968750e-02, -2.08251953e-01,\n",
              "       -9.37500000e-02, -1.91162109e-01,  1.14257812e-01,  7.66601562e-02,\n",
              "       -1.72950745e-01, -2.79296875e-01,  4.19921875e-02, -1.67480469e-01,\n",
              "        1.38671875e-01, -2.63916016e-01, -3.86962891e-01, -9.93652344e-02,\n",
              "        6.90917969e-02, -1.40136719e-01, -1.16210938e-01,  2.97851562e-02,\n",
              "       -1.07421875e-01,  4.19921875e-02, -2.07641602e-01, -1.46484375e-02],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_vectors['airplane']-word_vectors['flight']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2n5nG5HuxiO"
      },
      "source": [
        "Utilizando o método ```most_similar```, podemos retornar as palavras mais similares por meio de uma operação aritmética:\n",
        "somam-se os vetores positivos e subtrai o vetor negativo. A partir do resultado, podemos obter as palavras mais similares comparando-se esse vetor resultante com os vetores das demais palavras do *word embedding* com base na similaridade cosseno. Abaixo as top 10 palavras mais similares são mostradas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeVX-HpjujY-",
        "outputId": "21324e6b-4170-4a15-bdfe-3a2605ead7a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('plane', 0.6277297735214233),\n",
              " ('jet', 0.5784463882446289),\n",
              " ('flights', 0.5631440877914429),\n",
              " ('airliner', 0.5585241913795471),\n",
              " ('aircraft', 0.5546182990074158),\n",
              " ('jetliner', 0.550014853477478),\n",
              " ('NOTE_Expedia_Expedia.com', 0.5478827357292175),\n",
              " ('airplanes', 0.5451778173446655),\n",
              " ('Flight', 0.5407993197441101),\n",
              " ('airline', 0.5332231521606445)]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_vectors.most_similar(positive=['airplane','flight'],negative=['ship'],topn=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9c97-A8vK6S"
      },
      "source": [
        "#  Desafio\n",
        "\n",
        " Reproduza os experimentos importando os *embeddings* treinados do Nilc disponíveis em: [http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc](http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc). Utilize vetores de tamanho 50 para não inviabilizar seus testes."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

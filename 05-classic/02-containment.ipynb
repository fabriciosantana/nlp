{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriciosantana/nlp/blob/main/AKCIT_NLP_M6_Colab_Unidade_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfevqw7Eyf76"
      },
      "source": [
        "# Objetivos de Aprendizagem\n",
        "\n",
        "*  Esse notebook é um tutorial breve sobre formas clássicas de representação de palavras. Para exemplicar, iremos implementar uma função *containment*, uma função que irá comparar dois textos e analisar a similaridade dos mesmos com relação aos seus n-gramas de interseção. Primeiramente iremos entender os conceitos de vocabulário, n-gramas para posteriormente implementar a função.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJA5knmZwYFH"
      },
      "source": [
        "\n",
        "## 2.1 Contar N-gram\n",
        "\n",
        "Primeiramente temos que contar as ocorrências de n-gramas dos nossos textos. Usaremos o CountVectorizer para converter nosso corpus em uma matriz.\n",
        "\n",
        "No código abaixo, podemos variar o valor de n e utilizar o CountVectorizer para contar as ocorrências de n gramas. Podemos notar que na célula abaixo estamos criando um vocabulário através da utilização do CountVectorizer e, posteriormente, iremos analisar a matriz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvZsiZ_9xRbd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJffIQZrnmjh"
      },
      "source": [
        "### 2.1.1 Unigrama\n",
        "A execução do exemplo imprime o vocabulário. Podemos ver que existem 8 palavras(tokens) no vocabulário e, portanto, vetores codificados também possuem um comprimento de 8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ_oSuE0n0hh",
        "outputId": "a732e0b4-d623-4930-874a-7fa8c45d6656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'suponha': 11, 'que': 8, 'esse': 4, 'seja': 10, 'texto': 12, 'desejo': 2, 'comparar': 1, 'essa': 3, 'principal': 7, 'nada': 6, 'ver': 13, 'com': 0, 'eu': 5, 'quero': 9}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "texto_a_ser_comparado = \"Suponha que esse seja o texto que desejo comparar\"\n",
        "texto_fonte = \"Suponha que essa seja o texto principal\"\n",
        "texto_fonte2 = \"Texto nada a ver com o que eu quero\"\n",
        "\n",
        "# Número de n_gramas\n",
        "n = 1\n",
        "\n",
        "# Instancia o contador de n-gramas\n",
        "counts = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
        "\n",
        "# Cria um dicionário de n-gramas\n",
        "vocab2int = counts.fit([texto_a_ser_comparado, texto_fonte, texto_fonte2]).vocabulary_\n",
        "\n",
        "# Imprime dicionário de palavras:index\n",
        "print(vocab2int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kip_stY7n7os"
      },
      "source": [
        "### 2.2.2 Bigrama\n",
        "O mesmo vale para o caso de bigramas. Temos 8 bigramas no vocabulário e, portanto, os vetores codificados com comprimento 8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGFqH6AWn_so",
        "outputId": "bb51aba2-593d-4fb9-9fdc-0cdb4fc0d73e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'suponha que': 11, 'que esse': 8, 'esse seja': 3, 'seja texto': 10, 'texto que': 14, 'que desejo': 6, 'desejo comparar': 1, 'que essa': 7, 'essa seja': 2, 'texto principal': 13, 'texto nada': 12, 'nada ver': 5, 'ver com': 15, 'com que': 0, 'que eu': 9, 'eu quero': 4}\n"
          ]
        }
      ],
      "source": [
        "# Número de n_gramas\n",
        "n = 2\n",
        "\n",
        "# Instancia o contador de n-gramas\n",
        "counts = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
        "\n",
        "# Cria um dicionário de n-gramas\n",
        "vocab2int = counts.fit([texto_a_ser_comparado, texto_fonte, texto_fonte2]).vocabulary_\n",
        "\n",
        "# Imprime dicionário de palavraś:index\n",
        "print(vocab2int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkj8ljFtoDJw"
      },
      "source": [
        "### 2.2.3 Trigrama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hlWKpY5oGf3",
        "outputId": "9d175410-6234-40b9-d594-daf54556cdb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'suponha que esse': 11, 'que esse seja': 6, 'esse seja texto': 2, 'seja texto que': 9, 'texto que desejo': 13, 'que desejo comparar': 4, 'suponha que essa': 10, 'que essa seja': 5, 'essa seja texto': 1, 'seja texto principal': 8, 'texto nada ver': 12, 'nada ver com': 3, 'ver com que': 14, 'com que eu': 0, 'que eu quero': 7}\n"
          ]
        }
      ],
      "source": [
        "# Número de n_gramas\n",
        "n = 3\n",
        "\n",
        "# Instancia o contador de n-gramas\n",
        "counts = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
        "\n",
        "# Cria um dicionário de n-gramas\n",
        "vocab2int = counts.fit([texto_a_ser_comparado, texto_fonte, texto_fonte2]).vocabulary_\n",
        "\n",
        "# Imprime dicionário de palavraś:index\n",
        "print(vocab2int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeAMz9Y0oJiC"
      },
      "source": [
        "### 2.2.4 As palavras do vocabulário\n",
        "Note que o artigo \"o\" das `frases texto_a_ser_comparado` e `texto_fonte` não aparece no vocabulário. Note ainda que todas as frases encontram-se em minúsculo. Isso ocorre devido ao fato de que quando passamos o parâmetro `analyser = 'word'`, estamos considerando em nossa análise palavras com dois ou mais caracteres e consequentemente ignorando as palavras com apenas um caracter. Excluir esses caracteres (artigos) é um comportamento padrão e desejado em muitas análises de texto devido a sua irrelevância, em grande parte das análises textuais.\n",
        "\n",
        "Caso você precise desconsiderar o padrão *default* do CountVectorizer e adicionar palavras com caracteres únicos em sua análise, você pode adicionar o argumento `token_pattern = r\"(?u)\\b\\w+\\b\"`. Essa expressão regular (REGEX) define palavra como tendo uma ou mais caracteres."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SWfmE5toWLL"
      },
      "source": [
        "# 2.3 Array de n-gramas\n",
        "Vamos usar o CountVectorizer para criar um array com as contagens de n-gramas. Além disso, vamos criar duas sentenças que desejamos analizar, e transformar cada texto em um vetor numérico representando a ocorrência de cada palavra.\n",
        "\n",
        "Notar que cada linha representa um texto e cada coluna ou index representa os termos do vocabulário. Iremos ver isso claramente no mapeamento abaixo.\n",
        "\n",
        "* texto_a_ser_comparado =  \"Suponha que essa seja o texto que desejo comparar\"\n",
        "* texto_fonte =  \"Suponha que essa seja o texto principal\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOeqmDFBoUO5",
        "outputId": "c68bf66d-fd0e-49fd-b55a-73a6ea93f0a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vetor de n-gramas:\n",
            "\n",
            " [[0 1 1 0 1 0 0 0 2 0 1 1 1 0]\n",
            " [0 0 0 1 0 0 0 1 1 0 1 1 1 0]\n",
            " [1 0 0 0 0 1 1 0 1 1 0 0 1 1]]\n",
            "\n",
            "Dicionário de n-gramas (unigrama):\n",
            "\n",
            " {'suponha': 11, 'que': 8, 'esse': 4, 'seja': 10, 'texto': 12, 'desejo': 2, 'comparar': 1, 'essa': 3, 'principal': 7, 'nada': 6, 'ver': 13, 'com': 0, 'eu': 5, 'quero': 9}\n"
          ]
        }
      ],
      "source": [
        "# N-gramas\n",
        "n = 1\n",
        "\n",
        "# Instancia o contador de n-gramas\n",
        "counts = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
        "\n",
        "# cria uma matriz de contagem de n-grama para os dois textos\n",
        "n_grams = counts.fit_transform([texto_a_ser_comparado, texto_fonte, texto_fonte2])\n",
        "\n",
        "# Cria um dicionário de n-gramas\n",
        "vocab2int = counts.fit([texto_a_ser_comparado, texto_fonte, texto_fonte2]).vocabulary_\n",
        "\n",
        "n_grams_array = n_grams.toarray()\n",
        "\n",
        "print('Vetor de n-gramas:\\n\\n', n_grams_array)\n",
        "print()\n",
        "print('Dicionário de n-gramas (unigrama):\\n\\n', vocab2int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S215Qt3voufv"
      },
      "outputs": [],
      "source": [
        "texto_a_ser_comparado = \"Suponha que essa seja o texto que desejo comparar\"\n",
        "texto_fonte = \"Suponha que essa seja o texto principal\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx3z_SjtoqFU"
      },
      "source": [
        "Acima temos os vetores que codificam cada texto. Na linha superior temos os n-gramas do `text_a_ser_comparado` e na linha inferior temos a codificação do `text_fonte`. Podemos analisar se os textos possuem n_gramas em comum através de suas colunas. Por exemplo, ambos possuem a palavra `texto` (índice 7 - ultima coluna coluna). O mesmo vale para os unigramas `[essa]`, `[seja]`, `[que]` e `[suponha]`. Notar que o unigrama `[que]` ocorre duas vezes no segundo texto.\n",
        "\n",
        "\n",
        "```\n",
        "[[1 1 1 0 2 1 1 1]    =   comparar  desejo [essa] _________ [que] [seja] [suponha] [texto]\n",
        " [0 0 1 1 1 1 1 1]]   =   ________  ______ [essa] principal [que] [seja] [suponha] [texto]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlcnT_ygojEo"
      },
      "source": [
        " # 2.4 Valores de *containment*\n",
        "O *Containment* nada mais é do que uma medida de similaridade entre textos. É basicamente uma normalização da interseção da contagem de n-gramas entre os textos.\n",
        "\n",
        "\n",
        "Primeiro, precisamos extrair as palavras dos dois documentos de texto para formar um corpus. Em seguida, contamos a interseção de n-gramas (agrupamentos sequenciais de palavras de n palavras) entre os textos. Para o caso de unigramas, podemos considerar como uma contagem  dos número de palavras que ambos os textos têm em comum.\n",
        "\n",
        "Em seguida, dividimos o valor pelo total de n-gramas do texto a ser comparado (subíndice A - o qual quer ser comparado com o texto fonte) para normalizar o valor.\n",
        "\n",
        "\n",
        "Cálculo de *Containment*:\n",
        "\n",
        "1. Calcular a interseção n-grama entre o texto e o texto fonte.\n",
        "2. Adicionar o número de termos comuns.\n",
        "3. Normalizar o valor na etapa 2 pelo número de n gramas no texto A.\n",
        "\n",
        "\n",
        "Abaixo podemos ver a equação de *Containment*:\n",
        "$$ \\frac{\\sum{count(\\text{ngram}_{A}) \\cap count(\\text{ngram}_{F})}}{\\sum{count(\\text{ngram}_{A})}} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us1qY0KVpJnU"
      },
      "source": [
        "#2.4.1 Vamos criar uma função que recebe um array n-gramas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23U9cyjXpFbJ"
      },
      "outputs": [],
      "source": [
        "def containment(n_gram_array):\n",
        "    ''' Calcula o containment entre dois textos. Normaliza a interseção dos contadores de n-gramas\n",
        "    entre os textos.\n",
        "    ARG:\n",
        "    n_gra_array(array): Um array com as contagens de n-gramas dos dois textos a serem comparados\n",
        "\n",
        "    RETURNS:\n",
        "    O valor de containment normalizado '''\n",
        "\n",
        "\n",
        "     # Cria uma lista que contém o valor mínimo encontrado nas colunas\n",
        "     # 0 se não houver correspondências e 1+ para as palavras correspondentes\n",
        "\n",
        "    intersection_list = np.amin(n_gram_array, axis = 0)\n",
        "\n",
        "    # Soma número de interseção\n",
        "    intersection_count = np.sum(intersection_list)\n",
        "\n",
        "    # Conta número de n-gramas no texto 1\n",
        "    A_idx = 0\n",
        "    A_count = np.sum(n_gram_array[A_idx])\n",
        "\n",
        "\n",
        "    # Normaliza e calcula valor final\n",
        "    containment_val = intersection_count / A_count\n",
        "\n",
        "    return containment_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZCiqX6PpTev"
      },
      "source": [
        "#### Para o n_gram calculado anteriormente e n = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wypo2GalpVwM",
        "outputId": "1648718e-dd4a-4d69-b2c8-d70f32e27fcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Containment:  0.25\n"
          ]
        }
      ],
      "source": [
        "containment_val = containment(n_grams.toarray())\n",
        "\n",
        "print('Containment: ', containment_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxdCRLfQpaAC"
      },
      "source": [
        "#### para n = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBn3Q4JRpYX7",
        "outputId": "a1563dfa-b5e2-45bb-fb05-4396bbe5f6ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Containment for n=2 :  0.5714285714285714\n"
          ]
        }
      ],
      "source": [
        "counts_2grams = CountVectorizer(analyzer='word', ngram_range=(2,2))\n",
        "bigram_counts = counts_2grams.fit_transform([texto_a_ser_comparado, texto_fonte])\n",
        "\n",
        "# Calcula containment\n",
        "containment_val = containment(bigram_counts.toarray())\n",
        "\n",
        "print('Containment for n=2 : ', containment_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PwQYVtwpljB"
      },
      "source": [
        "# 2.4.2 Exercício:\n",
        "Teste a função com diferentes textos , n-gramas e tente imaginar aplicações desse conceito. Por exemplo, podemos usar essa técnica como uma métrica de análise de similaridade para detectar plagiarismo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAzmKFKPpqGl"
      },
      "source": [
        "# 2.5 Similaridade Cosseno\n",
        "\n",
        "Outra métrica usada para cálculo de similaridade entre textos é o cosseno. Quem já cursou álgebra linear deve lembrar que o produto escalar de dois vetores é igual ao módulo de cada vetor vezes o cosseno do ângulo formado entre eles. E é justamente o cosseno que utilizaremos para determinar o quanto similar são os textos. Lembrando que a função cosseno vai do intervalo fechado -1 à 1, sendo que, quando:\n",
        "\n",
        "*   cosseno = 1 -> Os vetores são paralelos e com mesmo sentido,\n",
        "*   cosseno = 0 -> Os vetores são perpendiculares,\n",
        "* cosseno = -1 -> Os vetores são paralelos e com sentido oposto.\n",
        "\n",
        "De maneira simples, quanto maior for o valor do cosseno mais similar são as sentenças. Vamos testar?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Lo7lOWlp5pt",
        "outputId": "2814f09c-0bc1-472b-c639-10f61419621d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.]])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "texto_a_ser_comparado = \"Suponha que esse seja o texto que desejo comparar\"\n",
        "texto_fonte = \"Suponha que essa seja o texto principal\"\n",
        "texto_fonte2 = \"Texto nada a ver com o que eu quero\"\n",
        "texto_fonte3 = \"Suponha que esse seja o texto que desejo comparar\"\n",
        "\n",
        "# N-gramas\n",
        "n = 1\n",
        "\n",
        "# Instancia o contador de n-gramas\n",
        "counts = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
        "\n",
        "# cria uma matriz de contagem de n-grama para os dois textos\n",
        "x = counts.fit_transform([texto_a_ser_comparado, texto_fonte, texto_fonte2, texto_fonte3])\n",
        "\n",
        "cosine_similarity(x[0],x[3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1meNyihHqgPf"
      },
      "source": [
        "#  2.6 Desafios\n",
        "\n",
        "1.  Vamos implementar um rankeador de documentos? O desafio é, dada uma *string* de busca denominada \"query\", rankear quais os documentos são mais similares em ordem de similaridade, usando a similaridade cosseno. Realize os seguintes Pré-processamentos de Texto: Tokenização, Remova as *stopwords*, Lematize as palavras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPC7pqHpqbDb"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    \"Natural Language Processing (NLP) enables computers to understand human language.\",\n",
        "    \"Machine learning provides systems the ability to automatically learn and improve from experience.\",\n",
        "    \"Deep learning is a subset of machine learning involving neural networks with three or more layers.\",\n",
        "    \"Natural Language Processing includes tasks like text generation and sentiment analysis.\",\n",
        "    \"Supervised learning involves learning from a training dataset with labeled data.\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJNt3ctZrkR0"
      },
      "source": [
        "##Perguntas para Reflexão:\n",
        "1. Qual é a importância de pré-processar os textos antes de calcular a similaridade?\n",
        "\n",
        "*O pré-processamento de textos é importante para garantir que os dados estejam limpos, consistentes e prontos para serem analisados/ usados em outros algoritmos, por exemplo, análise de sentimentos, classificação, análise de tópicos. O pré-processamento de textos envolve etapas como remoção de stop words, stemming, lematização, normalização de maiúsculas/minúsculas e remoção de caracteres especiais. *\n",
        "\n",
        "Mas já parou para pensar que ao remover artigos ou preprosições podemos perder a semântica de expressões importantes para o contexto?\n",
        "\n",
        "2. Como a vetorização ajuda a capturar a importância das palavras nos documentos?\n",
        "\n",
        "*A vetorização converte textos em representações numéricas, geralmente vetores de palavras ou tokens. Técnicas como TF-IDF (Term Frequency-Inverse Document Frequency) ajudam a atribuir pesos às palavras com base em sua frequência no documento e no corpus, destacando termos mais representativos. Isso evita que palavras muito comuns (como \"de\", \"e\", \"é\") tenham peso excessivo, focando nos termos que ajudam a diferenciar um documento de outro. Vetores gerados por técnicas como embeddings de palavras (Word2Vec, BERT) também capturam relações semânticas entre palavras.*\n",
        "\n",
        "3. Por que a similaridade cosseno é uma métrica apropriada para medir a similaridade entre documentos de texto?\n",
        "\n",
        "*A similaridade cosseno mede o ângulo entre dois vetores, independentemente de sua magnitude, sendo especialmente útil quando os documentos têm comprimentos variáveis. Essa métrica se concentra na direção dos vetores (ou seja, nos padrões de coocorrência de palavras) em vez de suas magnitudes absolutas, o que ajuda a captar a semelhança no conteúdo, ignorando diferenças triviais no tamanho dos documentos. Como o valor resultante varia entre -1 e 1, onde 1 significa alta similaridade e 0 significa ausência de similaridade, é uma forma eficiente de comparar documentos de texto.*\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
